[
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/code_processor.py",
    "language": "python",
    "code_blocks": [
      "\"\"\"\n代码处理器模块，负责代码的加载、分割和向量化。\n\"\"\"\nimport os\nfrom typing import List, Optional, Dict, Any\nfrom pathlib import Path\n\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\nfrom llama_index.core.node_parser import CodeSplitter\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.vector_stores.lancedb import LanceDBVectorStore\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\nimport lancedb",
      "class CodeProcessor:",
      "\"\"\"代码处理器类，用于处理代码仓库中的代码。\"\"\"\n    \n    # 支持的编程语言映射\n    LANGUAGE_MAP = {\n        '.py': 'python',\n        '.js': 'javascript',\n        '.ts': 'typescript',\n        '.java': 'java',\n        '.cpp': 'cpp',\n        '.c': 'c',\n        '.go': 'go',\n        '.rs': 'rust',\n        '.rb': 'ruby',\n        '.php': 'php',\n        '.swift': 'swift',\n        '.kt': 'kotlin',\n        '.scala': 'scala',\n        '.hs': 'haskell',\n        '.lua': 'lua',\n        '.sh': 'bash',\n        '.r': 'r',\n        '.jl': 'julia',\n        '.pl': 'perl',\n        '.sql': 'sql',\n    }\n    \n    # 支持的代码文件扩展名\n    SUPPORTED_EXTENSIONS = list(LANGUAGE_MAP.keys())",
      "def __init__(\n        self,\n        code_repo_dir: str,\n        lancedb_uri: str = \"./lancedb_data\",\n        lancedb_table_name: str = \"code_snippets\",\n        embedding_model_name: str = \"Qwen/Qwen3-Embedding-0.6B\",\n        rerank_model_name: str = \"Qwen/Qwen3-Reranker-0.6B\",\n        top_k_rerank: int = 3,\n        chunk_lines: int = 40,\n        chunk_lines_overlap: int = 15,\n        max_chars: int = 1500,\n    ):",
      "\"\"\"\n        初始化代码处理器。\n\n        Args:\n            code_repo_dir: 代码仓库目录\n            lancedb_uri: LanceDB 数据库路径\n            lancedb_table_name: LanceDB 表名\n            embedding_model_name: 嵌入模型名称\n            rerank_model_name: 重排序模型名称\n            top_k_rerank: 重排序后保留的结果数量\n            chunk_lines: 代码块最大行数\n            chunk_lines_overlap: 代码块重叠行数\n            max_chars: 代码块最大字符数\n        \"\"\"\n        self.code_repo_dir = Path(code_repo_dir)\n        self.lancedb_uri = lancedb_uri\n        self.lancedb_table_name = lancedb_table_name\n        \n        # 初始化模型和处理器\n        self._init_models(\n            embedding_model_name=embedding_model_name,\n            rerank_model_name=rerank_model_name,\n            top_k_rerank=top_k_rerank\n        )\n        \n        # 初始化代码分割器\n        self._init_code_splitter(\n            chunk_lines=chunk_lines,\n            chunk_lines_overlap=chunk_lines_overlap,\n            max_chars=max_chars\n        )\n        \n        # 初始化向量存储\n        self._init_vector_store()\n        \n        self.index = None",
      "def _init_models(\n        self,\n        embedding_model_name: str,\n        rerank_model_name: str,\n        top_k_rerank: int\n    ) -> None:\n        \"\"\"\n        初始化嵌入模型和重排序模型。\n\n        Args:\n            embedding_model_name: 嵌入模型名称\n            rerank_model_name: 重排序模型名称\n            top_k_rerank: 重排序后保留的结果数量\n        \"\"\"\n        # 初始化嵌入模型\n        self.embed_model = HuggingFaceEmbedding(model_name=embedding_model_name)\n        Settings.embed_model = self.embed_model\n        \n        # 初始化重排序模型\n        self.reranker = SentenceTransformerRerank(\n            model=rerank_model_name,\n            top_n=top_k_rerank\n        )\n\n    def _init_code_splitter(\n        self,\n        chunk_lines: int,\n        chunk_lines_overlap: int,\n        max_chars: int\n    ) -> None:\n        \"\"\"\n        初始化代码分割器。\n\n        Args:\n            chunk_lines: 代码块最大行数\n            chunk_lines_overlap: 代码块重叠行数\n            max_chars: 代码块最大字符数\n        \"\"\"\n        self.code_splitter = CodeSplitter(\n            language=\"python\",  # 默认使用 Python，后续会根据文件类型自动判断\n            chunk_lines=chunk_lines,\n            chunk_lines_overlap=chunk_lines_overlap,\n            max_chars=max_chars,\n        )\n\n    def _init_vector_store(self) -> None:\n        \"\"\"初始化 LanceDB 向量存储。\"\"\"\n        self.db = lancedb.connect(self.lancedb_uri)\n        self.vector_store = LanceDBVectorStore(\n            uri=self.lancedb_uri,\n            table_name=self.lancedb_table_name,\n            db=self.db\n        )",
      "def _detect_language(self, file_path: Path) -> str:\n        \"\"\"\n        根据文件扩展名检测编程语言。\n\n        Args:\n            file_path: 文件路径\n\n        Returns:\n            str: 编程语言名称\n        \"\"\"\n        extension = file_path.suffix.lower()\n        return self.LANGUAGE_MAP.get(extension, 'python')  # 默认使用 Python\n\n    def process_code_repo(self) -> int:\n        \"\"\"\n        处理整个代码仓库，构建向量索引。\n\n        Returns:\n            int: 处理的代码片段数量\n        \"\"\"\n        if not self.code_repo_dir.exists():\n            raise ValueError(f\"代码仓库目录不存在: {self.code_repo_dir}\")\n\n        # 加载所有代码文件\n        documents = SimpleDirectoryReader(\n            input_dir=str(self.code_repo_dir),\n            recursive=True,\n            required_exts=self.SUPPORTED_EXTENSIONS\n        ).load_data()\n\n        # 处理每个文档\n        all_nodes = []\n        for doc in documents:\n            file_path = Path(doc.metadata.get('file_path', ''))\n            language = self._detect_language(file_path)\n            \n            # 更新代码分割器的语言设置\n            self.code_splitter.language = language\n            \n            # 分割代码\n            nodes = self.code_splitter.get_nodes_from_documents([doc])\n            all_nodes.extend(nodes)\n\n        # 构建向量索引\n        self.index = VectorStoreIndex(all_nodes, vector_store=self.vector_store)\n        return len(all_nodes)",
      "def query_code(self, query: str, similarity_top_k: int = 10) -> Dict[str, Any]:\n        \"\"\"\n        查询代码。\n\n        Args:\n            query: 查询文本\n            similarity_top_k: 初步召回的结果数量\n\n        Returns:\n            Dict[str, Any]: 查询结果，包含生成的摘要和源代码片段\n        \"\"\"\n        if not self.index:\n            raise ValueError(\"请先调用 process_code_repo() 处理代码仓库\")\n\n        query_engine = self.index.as_query_engine(\n            similarity_top_k=similarity_top_k,\n            node_postprocessors=[self.reranker]\n        )\n\n        response = query_engine.query(query)\n        \n        return {\n            \"summary\": response.response,\n            \"source_nodes\": [\n                {\n                    \"file_path\": node.metadata.get('file_path', '未知文件'),\n                    \"score\": node.score,\n                    \"content\": node.text\n                }\n                for node in response.source_nodes\n            ]\n        }\n\n    def cleanup(self) -> None:\n        \"\"\"清理 LanceDB 数据库。\"\"\"\n        if os.path.exists(self.lancedb_uri):\n            import shutil\n            shutil.rmtree(self.lancedb_uri)"
    ]
  },
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/example_utils.py",
    "language": "python",
    "code_blocks": [
      "# example_utils.py\n\ndef calculate_sum(numbers: list) -> int:\n    \"\"\"\n    计算列表中所有数字的总和。\n\n    Args:\n        numbers (list): 包含整数或浮点数的列表。\n\n    Returns:\n        int: 列表中所有数字的和。\n    \"\"\"\n    total = 0\n    for num in numbers:\n        total += num\n    return total\n\ndef get_current_working_directory() -> str:\n    \"\"\"\n    使用 os 模块获取当前工作目录。\n\n    Returns:\n        str: 当前工作目录的路径。\n    \"\"\"\n    import os\n    return os.getcwd()\n\nclass FileHandler:\n    \"\"\"\n    文件处理类，用于读写文件。\n    \"\"\"\n    def __init__(self, filepath: str):\n        self.filepath = filepath\n\n    def read_file(self) -> str:\n        \"\"\"读取文件内容。\"\"\"\n        with open(self.filepath, 'r', encoding='utf-8') as f:\n            content = f.read()\n        return content\n\n    def write_to_file(self, content: str):\n        \"\"\"将内容写入文件。\"\"\"\n        with open(self.filepath, 'w', encoding='utf-8') as f:\n            f.write(content)"
    ]
  },
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/rerank.py",
    "language": "python",
    "code_blocks": [
      "# Requires transformers>=4.51.0\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n\ndef format_instruction(instruction, query, doc):\n    if instruction is None:\n        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n    return output\n\ndef process_inputs(pairs):\n    inputs = tokenizer(\n        pairs, padding=False, truncation='longest_first',\n        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n    )\n    for i, ele in enumerate(inputs['input_ids']):\n        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n    for key in inputs:\n        inputs[key] = inputs[key].to(model.device)\n    return inputs\n\n@torch.no_grad()\ndef compute_logits(inputs, **kwargs):\n    batch_scores = model(**inputs).logits[:, -1, :]\n    true_vector = batch_scores[:, token_true_id]\n    false_vector = batch_scores[:, token_false_id]\n    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n    scores = batch_scores[:, 1].exp().tolist()\n    return scores\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')",
      "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\ntoken_false_id = tokenizer.convert_tokens_to_ids(\"no\")\ntoken_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\nmax_length = 8192\n\nprefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\nsuffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\nprefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\nsuffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n        \ntask = 'Given a web search query, retrieve relevant passages that answer the query'\n\nqueries = [\"What is the capital of China?\",\n    \"Explain gravity\",\n]\n\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n\npairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n\n# Tokenize the input texts\ninputs = process_inputs(pairs)\nscores = compute_logits(inputs)\n\nprint(\"scores: \", scores)"
    ]
  },
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/embedding.py",
    "language": "python",
    "code_blocks": [
      "# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to \"left\":\n# model = SentenceTransformer(\n#     \"Qwen/Qwen3-Embedding-0.6B\",\n#     model_kwargs={\"attn_implementation\": \"flash_attention_2\", \"device_map\": \"auto\"},\n#     tokenizer_kwargs={\"padding_side\": \"left\"},\n# )\n\n# The queries and documents to embed\nqueries = [\n    \"What is the capital of China?\",\n    \"Explain gravity\",\n]\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7646, 0.1414],\n#         [0.1355, 0.6000]])"
    ]
  },
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/test_code.py",
    "language": "python",
    "code_blocks": [
      "\"\"\"\n这是一个用于测试代码分割器的示例文件。\n包含了各种函数、类、装饰器等Python特性。\n\"\"\"\n\nimport time\nimport random\nfrom typing import List, Dict, Any, Optional, Union\nfrom dataclasses import dataclass\nfrom functools import wraps\nimport logging\nimport json\nimport os\nfrom pathlib import Path\n\n# 配置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef timing_decorator(func):\n    \"\"\"计算函数执行时间的装饰器\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        logger.info(f\"函数 {func.__name__} 执行时间: {end_time - start_time:.2f} 秒\")\n        return result\n    return wrapper\n\n@dataclass\nclass DataPoint:\n    \"\"\"数据点类\"\"\"\n    x: float\n    y: float\n    label: str\n    metadata: Dict[str, Any]",
      "class DataProcessor:",
      "\"\"\"数据处理类\"\"\"\n    \n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.data_points: List[DataPoint] = []\n        self.processed_data: Dict[str, List[float]] = {}\n    \n    @timing_decorator\n    def load_data(self, file_path: str) -> None:\n        \"\"\"加载数据\"\"\"\n        try:\n            with open(file_path, 'r') as f:\n                raw_data = json.load(f)\n            \n            for item in raw_data:\n                point = DataPoint(\n                    x=item['x'],\n                    y=item['y'],\n                    label=item['label'],\n                    metadata=item.get('metadata', {})\n                )\n                self.data_points.append(point)\n                \n            logger.info(f\"成功加载 {len(self.data_points)} 个数据点\")\n        except Exception as e:\n            logger.error(f\"加载数据失败: {str(e)}\")\n            raise\n    \n    def process_data(self) -> Dict[str, List[float]]:\n        \"\"\"处理数据\"\"\"\n        results = {\n            'x_values': [],\n            'y_values': [],\n            'distances': []\n        }\n        \n        for point in self.data_points:\n            results['x_values'].append(point.x)\n            results['y_values'].append(point.y)\n            distance = (point.x ** 2 + point.y ** 2) ** 0.5\n            results['distances'].append(distance)\n        \n        self.processed_data = results\n        return results",
      "def save_results(self, output_path: str) -> None:\n        \"\"\"保存处理结果\"\"\"\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        with open(output_path, 'w') as f:\n            json.dump(self.processed_data, f, indent=2)\n        logger.info(f\"结果已保存到: {output_path}\")",
      "class DataAnalyzer:",
      "\"\"\"数据分析类\"\"\"\n    \n    def __init__(self, data: Dict[str, List[float]]):\n        self.data = data\n        self.statistics: Dict[str, Dict[str, float]] = {}\n    \n    def calculate_statistics(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"计算统计数据\"\"\"\n        for key, values in self.data.items():\n            self.statistics[key] = {\n                'mean': sum(values) / len(values),\n                'min': min(values),\n                'max': max(values),\n                'std': self._calculate_std(values)\n            }\n        return self.statistics\n    \n    def _calculate_std(self, values: List[float]) -> float:\n        \"\"\"计算标准差\"\"\"\n        mean = sum(values) / len(values)\n        squared_diff_sum = sum((x - mean) ** 2 for x in values)\n        return (squared_diff_sum / len(values)) ** 0.5\n    \n    def generate_report(self, output_path: str) -> None:\n        \"\"\"生成分析报告\"\"\"\n        report = {\n            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n            'statistics': self.statistics,\n            'summary': self._generate_summary()\n        }\n        \n        with open(output_path, 'w') as f:\n            json.dump(report, f, indent=2)\n        logger.info(f\"分析报告已保存到: {output_path}\")\n    \n    def _generate_summary(self) -> Dict[str, Any]:\n        \"\"\"生成数据摘要\"\"\"\n        return {\n            'total_points': len(next(iter(self.data.values()))),\n            'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S'),\n            'data_quality': self._assess_data_quality()\n        }",
      "def _assess_data_quality(self) -> str:\n        \"\"\"评估数据质量\"\"\"\n        # 这里只是一个示例，实际应用中可能需要更复杂的评估逻辑\n        return \"良好\" if len(self.data) > 0 else \"未知\"",
      "def main():\n    \"\"\"主函数\"\"\"\n    # 创建测试数据\n    test_data = {\n        'x_values': [random.uniform(-10, 10) for _ in range(100)],\n        'y_values': [random.uniform(-10, 10) for _ in range(100)],\n        'distances': [random.uniform(0, 20) for _ in range(100)]\n    }\n    \n    # 保存测试数据\n    with open('test_data.json', 'w') as f:\n        json.dump(test_data, f, indent=2)\n    \n    # 创建数据处理器实例\n    processor = DataProcessor({'debug': True})\n    \n    try:\n        # 加载数据\n        processor.load_data('test_data.json')\n        \n        # 处理数据\n        processed_data = processor.process_data()\n        \n        # 创建分析器实例\n        analyzer = DataAnalyzer(processed_data)\n        \n        # 计算统计数据\n        statistics = analyzer.calculate_statistics()\n        \n        # 生成报告\n        analyzer.generate_report('analysis_report.json')\n        \n        logger.info(\"数据处理和分析完成\")\n        \n    except Exception as e:\n        logger.error(f\"处理过程中出错: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()"
    ]
  },
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/data_processing.py",
    "language": "python",
    "code_blocks": [
      "# data_processing.py\n\nimport pandas as pd\n\ndef load_csv(filepath: str) -> pd.DataFrame:\n    \"\"\"\n    加载 CSV 文件到 Pandas DataFrame。\n\n    Args:\n        filepath (str): CSV 文件的路径。\n\n    Returns:\n        pd.DataFrame: 加载的 DataFrame。\n    \"\"\"\n    df = pd.read_csv(filepath)\n    return df\n\ndef filter_dataframe(df: pd.DataFrame, column: str, value: any) -> pd.DataFrame:\n    \"\"\"\n    根据列值过滤 DataFrame。\n\n    Args:\n        df (pd.DataFrame): 输入 DataFrame。\n        column (str): 要过滤的列名。\n        value (any): 要匹配的值。\n\n    Returns:\n        pd.DataFrame: 过滤后的 DataFrame。\n    \"\"\"\n    return df[df[column] == value]\n\ndef save_dataframe_to_parquet(df: pd.DataFrame, filepath: str):\n    \"\"\"\n    将 DataFrame 保存为 Parquet 文件。\n\n    Args:\n        df (pd.DataFrame): 要保存的 DataFrame。\n        filepath (str): Parquet 文件的保存路径。\n    \"\"\"\n    df.to_parquet(filepath)"
    ]
  },
  {
    "file_path": "/Users/yuanzhi/CodeSage/my_code_repo_for_lancedb_demo/code_splitter.py",
    "language": "python",
    "code_blocks": [
      "\"\"\"\n代码切分器模块，提供基于 AST 的代码切分功能。\n\"\"\"\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nimport json\nimport time\nfrom datetime import datetime\n\nfrom llama_index.core import Document\nfrom llama_index.core.node_parser import CodeSplitter",
      "class ASTCodeSplitter:",
      "\"\"\"基于 AST 的代码切分器，支持多种编程语言。\"\"\"\n    \n    # 支持的编程语言映射\n    LANGUAGE_MAP = {\n        '.py': 'python',\n        '.js': 'javascript',\n        '.ts': 'typescript',\n        '.java': 'java',\n        '.cpp': 'cpp',\n        '.c': 'c',\n        '.go': 'go',\n        '.rs': 'rust',\n        '.rb': 'ruby',\n        '.php': 'php',\n        '.swift': 'swift',\n        '.kt': 'kotlin',\n        '.scala': 'scala',\n        '.hs': 'haskell',\n        '.lua': 'lua',\n        '.sh': 'bash',\n        '.r': 'r',\n        '.jl': 'julia',\n        '.pl': 'perl',\n        '.sql': 'sql',\n    }\n    \n    def __init__(\n        self,\n        chunk_lines: int = 40,\n        chunk_lines_overlap: int = 15,\n        max_chars: int = 1500,\n        language: str = \"python\"\n    ):\n        \"\"\"\n        初始化代码切分器。\n\n        Args:\n            chunk_lines: 代码块最大行数\n            chunk_lines_overlap: 代码块重叠行数\n            max_chars: 代码块最大字符数\n            language: 默认编程语言\n        \"\"\"\n        self.chunk_lines = chunk_lines\n        self.chunk_lines_overlap = chunk_lines_overlap\n        self.max_chars = max_chars\n        self.language = language\n        \n        # 初始化 LlamaIndex 的代码切分器\n        self._splitter = CodeSplitter(\n            language=language,\n            chunk_lines=chunk_lines,\n            chunk_lines_overlap=chunk_lines_overlap,\n            max_chars=max_chars\n        )",
      "def _detect_language(self, file_path: Path) -> str:\n        \"\"\"\n        根据文件扩展名检测编程语言。\n\n        Args:\n            file_path: 文件路径\n\n        Returns:\n            str: 编程语言名称\n        \"\"\"\n        extension = file_path.suffix.lower()\n        return self.LANGUAGE_MAP.get(extension, '')",
      "def split_directory(self, directory_path: str) -> List[Dict[str, Any]]:",
      "\"\"\"\n        处理文件夹中的所有代码文件，返回代码块信息。\n\n        Args:\n            directory_path: 文件夹路径\n\n        Returns:\n            List[Dict[str, Any]]: 代码块信息列表，每个字典包含：\n                - file_path: 文件路径\n                - language: 编程语言\n                - code_blocks: 代码块列表\n                - metadata: 元数据信息\n        \"\"\"\n        directory = Path(directory_path)\n        if not directory.exists() or not directory.is_dir():\n            raise ValueError(f\"目录不存在或不是有效的目录: {directory_path}\")\n\n        results = []\n        \n        # 递归遍历目录",
      "for file_path in directory.rglob(\"*\"):\n            if not file_path.is_file():\n                continue\n                \n            # 检测文件语言\n            language = self._detect_language(file_path)\n            if not language:\n                continue\n                \n            try:\n                # 读取文件内容\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                # 分割代码\n                nodes = self._splitter.split_text(content)\n                \n                # 处理分割结果\n                result = {\n                    \"file_path\": str(file_path),\n                    \"language\": language,\n                    \"code_blocks\": [str(node) for node in nodes],\n                    \"metadata\": {\n                        \"total_blocks\": len(nodes),\n                        \"file_size\": len(content),\n                        \"last_modified\": datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')\n                    }\n                }\n                results.append(result)\n                \n            except UnicodeDecodeError:\n                print(f\"警告：无法解码文件 {file_path}，跳过处理\")\n                continue\n            except Exception as e:\n                print(f\"处理文件 {file_path} 时出错：{str(e)}\")\n                continue\n\n        return results",
      "def process_code_repository(repo_path: str) -> Dict[str, Any]:\n    \"\"\"\n    处理代码仓库，返回处理结果。\n\n    Args:\n        repo_path: 代码仓库路径\n\n    Returns:\n        Dict[str, Any]: 处理结果统计信息\n    \"\"\"\n    # 初始化代码切分器\n    splitter = ASTCodeSplitter(\n        chunk_lines=40,\n        chunk_lines_overlap=15,\n        max_chars=1500\n    )\n\n    # 处理代码仓库\n    results = splitter.split_directory(repo_path)\n\n    return results",
      "def main():\n    \"\"\"主函数\"\"\"\n    # 获取当前文件所在目录\n    current_dir = Path(__file__).parent.parent.parent.parent\n    repo_path = current_dir / \"my_code_repo_for_lancedb_demo\"\n\n    if not repo_path.exists():\n        print(f\"错误：代码仓库路径不存在: {repo_path}\")\n        return\n\n    print(f\"开始处理代码仓库: {repo_path}\")\n    \n    try:\n        # 处理代码仓库\n        stats = process_code_repository(str(repo_path))\n        \n        # 将结果保存到 JSON 文件\n        output_file = repo_path / \"code_analysis_results.json\"\n        with open(output_file, 'w', encoding='utf-8') as f:\n            json.dump(stats, f, ensure_ascii=False, indent=2)\n        print(f\"\\n处理结果已保存到: {output_file}\")\n        \n        # 打印简要统计信息\n        total_files = len(stats)\n        total_blocks = sum(len(item[\"code_blocks\"]) for item in stats)\n        print(f\"\\n处理完成:\")\n        print(f\"总文件数: {total_files}\")\n        print(f\"总代码块数: {total_blocks}\")\n\n    except Exception as e:\n        print(f\"处理过程中出错: {str(e)}\")\n\nif __name__ == \"__main__\":\n    main()"
    ]
  }
]